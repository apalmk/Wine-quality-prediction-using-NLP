{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AML4_task1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwhE3eCH2LW7",
        "colab_type": "text"
      },
      "source": [
        "## *Applied Machine Learning*\n",
        "\n",
        "# *Assignment 3*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCPi2EYR2TJf",
        "colab_type": "text"
      },
      "source": [
        "***Teammates:***\n",
        "\n",
        "Karthik Rajaraman Iyer\n",
        "kr2859@columbia.edu\n",
        "\n",
        "Anjani Prasad Atluri\n",
        "aa4462@columbia.edu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJOh2n-Dw0TO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Installs\n",
        "!pip install xgboost\n",
        "!pip install --upgrade category_encoders"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHI6IXsY2Mu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#imports\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import make_column_transformer \n",
        "from category_encoders import TargetEncoder\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "from nltk.stem.snowball import EnglishStemmer\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.preprocessing import StandardScaler \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk import download\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from category_encoders import CatBoostEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk import word_tokenize     \n",
        "import numpy as np\n",
        "import nltk     \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from sklearn.model_selection import GridSearchCV \n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.metrics import accuracy_score, r2_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostRegressor,GradientBoostingRegressor,RandomForestRegressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3TwkTq4NcU7",
        "colab_type": "code",
        "outputId": "2b2e147a-9e8a-4276-df3a-28e8b2d730a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQy5WFAj1vdz",
        "colab_type": "code",
        "outputId": "96337ff8-a51f-4ea8-f92e-934111895c39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#mounting drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "235uAUbN2D0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data loading\n",
        "df = pd.read_csv('/content/gdrive/My Drive/Columbia Photos/winemag-data-130k-v2.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qdLzhJH9lvK",
        "colab_type": "text"
      },
      "source": [
        "#Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD5id7qy4IkC",
        "colab_type": "text"
      },
      "source": [
        "We are removing the designation, country, region_2, taster_twitter_handle, and Unnamed: 0 columns. designation column because it is a categorical column with too many unique values and there might be a possible leak of information. We are subsetting on the country column (taking only US) and removing it once we subsetted on it. We are removing the region_2 column as it has too many null values and the region_1 is giving necessary region information, and if we kept both there might be a possibility of colinearity. Twitter handle column as it is a unique column and it will leak target information. 'Unnamed: 0' is the index column. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLZg13rI3NBg",
        "colab_type": "code",
        "outputId": "e7352747-280e-454c-942c-19f2e5b88155",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#subsample to winde form the U.S.\n",
        "df = df[df['country']=='US']\n",
        "\n",
        "#Removing columns (designation, country, region_2, taster_twitter_handle,Unnamed: 0)\n",
        "columns = ['designation', 'country', 'region_2', 'taster_twitter_handle','Unnamed: 0']\n",
        "df.drop(columns, inplace=True, axis=1)\n",
        "\n",
        "print(\"Column name and the amount of missing data in that column\")\n",
        "\n",
        "#Seeing if there are NAs in rest of the columns\n",
        "print(df.isnull().sum(axis = 0))\n",
        "\n",
        "#removing outliers from the price column\n",
        "Q1 = df['price'].quantile(0.25)\n",
        "Q3 = df['price'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "df = df[~((df['price'] < (Q1 - 1.5 * IQR)) |(df['price'] > (Q3 + 1.5 * IQR)))]\n",
        "\n",
        "#Subset the data\n",
        "df1=df.sample(frac=0.5, random_state=0)\n",
        "#df1 = df.copy()\n",
        "\n",
        "#Splitting the dataset into target variables and the covariates\n",
        "y=pd.DataFrame(df1['points']).values\n",
        "X= df1.loc[:, df1.columns != 'points']\n",
        "\n",
        "#Splitting the dataset into traning and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Column name and the amount of missing data in that column\n",
            "description        0\n",
            "points             0\n",
            "price            239\n",
            "province           0\n",
            "region_1         278\n",
            "taster_name    16774\n",
            "title              0\n",
            "variety            0\n",
            "winery             0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoKi3bO0uBQk",
        "colab_type": "text"
      },
      "source": [
        "We have removed the outliers from the price column to make better predicitons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kGG9x_OxH8z",
        "colab_type": "text"
      },
      "source": [
        "### **Task 1.1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXTJALLLFbka",
        "colab_type": "code",
        "outputId": "dde9b29c-8649-4c33-b39a-9f76850e8b94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train1 = X_train.drop(['description','title'],inplace=False, axis=1)\n",
        "X_test1 = X_test.drop(['description','title'],inplace=False, axis=1)\n",
        "\n",
        "#Getting the categorical and the continuous columns of the dataframe\n",
        "cat_cols= list(X_train1.select_dtypes(object).columns)\n",
        "cont_cols= list(X_train1.columns[X_train1.dtypes !=object])\n",
        "cat_cols.remove('taster_name')\n",
        "cat_cols_on = ['taster_name']\n",
        "\n",
        "#dropping indices of the columns\n",
        "X_train1.reset_index(drop=True,inplace=True)\n",
        "\n",
        "cont_preprocessing = make_pipeline(SimpleImputer(strategy='median'),StandardScaler())\n",
        "\n",
        "cat_preprocessing = make_pipeline(SimpleImputer(strategy='constant',fill_value='np'),CatBoostEncoder(handle_missing='value',return_df=False),StandardScaler())\n",
        "\n",
        "cat_preprocessing_on = make_pipeline(SimpleImputer(strategy='constant',fill_value='NA'),OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "\n",
        "preprocess = make_column_transformer((cat_preprocessing, cat_cols), (cont_preprocessing,cont_cols),(cat_preprocessing_on,cat_cols_on))\n",
        "\n",
        "pipe = Pipeline([(\"pre\",preprocess),(\"regressor\",xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.7, learning_rate = 0.1,max_depth = 15, alpha = 0.1, n_estimators = 300,min_samples_split=7))])\n",
        "\n",
        "pipe_grid={}\n",
        "grid = GridSearchCV(pipe,param_grid=pipe_grid,cv=5,scoring=\"r2\", n_jobs=-1)\n",
        "grid.fit(X_train1,y_train)\n",
        "\n",
        "print(\"Baseline's score on the test set\", grid.best_estimator_.score(X_test1, y_test))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline's score on the test set 0.4192172244940921\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNTluxwguOGb",
        "colab_type": "text"
      },
      "source": [
        "We chose XGBoost Regressor as our baseline model. We have encoded the taster's name using one hot encoding and the rest of the categorical columns using catboost encoding. We have imputed all the missing data from all the columns and scaled the continuous column 'price'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtH4R3UsWutZ",
        "colab_type": "text"
      },
      "source": [
        "## **Task 1.2** \n",
        "\n",
        "### Simple Bag of Words Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TPwSPWe03Db",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Including the Lemmatizer from nltk by defining LemmaTokenizer class reference: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
        "\n",
        "class LemmaTokenizer:\n",
        "  def __init__(self):\n",
        "    self.wnl = WordNetLemmatizer()\n",
        "  def __call__(self, doc):\n",
        "    return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
        "\n",
        "#Making a Stemming function from nltk reference: https://stackoverflow.com/questions/36182502/add-stemming-support-to-countvectorizer-sklearn\n",
        "stemmer = EnglishStemmer()\n",
        "analyzer = CountVectorizer().build_analyzer()\n",
        "\n",
        "def stemmed_words(doc):\n",
        "    return (stemmer.stem(w) for w in analyzer(doc))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOds-_E1XHIs",
        "colab_type": "code",
        "outputId": "a9bf8c38-f9b2-4198-fee4-f14055955eee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Simple text-based model using bag of words and a linear model\n",
        "\n",
        "#Append the title column data to the description column data and making a new dataframe\n",
        "nX_train= list(X_train['description'] +' '+ X_train['title'])\n",
        "nX_test = list(X_test['description'] +' '+ X_test['title'])\n",
        "\n",
        "vect = CountVectorizer(stop_words=\"english\")\n",
        "teXt_train = vect.fit_transform(nX_train)\n",
        "teXt_test = vect.transform(nX_test)\n",
        "\n",
        "clf = Ridge(alpha=1).fit(teXt_train, y_train)\n",
        "print(\"testing score for plain bag of words model: \",clf.score(teXt_test,y_test))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing score for plain bag of words model:  0.6694834440072108\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rj916bREtjD",
        "colab_type": "text"
      },
      "source": [
        "## **Task 1.3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMhSEEa0NLih",
        "colab_type": "code",
        "outputId": "d321f289-49b3-4614-ac9e-a60535ea41f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#n-grams (1 to 4 grams) with stemmer\n",
        "\n",
        "gram14 = CountVectorizer(ngram_range=(1, 4), min_df=2, stop_words=\"english\",lowercase=True, analyzer=stemmed_words)\n",
        "X_train_grm14 = gram14.fit_transform(nX_train)\n",
        "lr_grm14 = Ridge(alpha=22).fit(X_train_grm14, y_train)\n",
        "X_test_grm14 = gram14.transform(nX_test)\n",
        "print(\"Testing error on 1-4 grams: \",lr_grm14.score(X_test_grm14, y_test))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing error on 1-4 grams:  0.7344068405323045\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oinveQXCu78D",
        "colab_type": "text"
      },
      "source": [
        "We found that the n-gram model gave best results when we chose the n-gram range as 1-4. We also removed stop words, and performed word stemming. We referred to the following page for adding word stemming https://stackoverflow.com/questions/36182502/add-stemming-support-to-countvectorizer-sklearn.\n",
        "\n",
        "We restricted the words to the ones that appeared in more than 2 documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smYVDKETCWPs",
        "colab_type": "code",
        "outputId": "2694ad7d-f478-49c1-8c99-b5c2bbf2b400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#character n-grams with tuning and with high regularization\n",
        "\n",
        "# high regularization because the no of features will increase and we shouldn't be giving high importance (coefficients to each of them)\n",
        "#It performs well with high regularization\n",
        "\n",
        "cgram = CountVectorizer(tokenizer=LemmaTokenizer(),ngram_range=(1, 3), min_df=3, stop_words=\"english\",lowercase=True,analyzer=\"char_wb\")\n",
        "X_train_cgrm = cgram.fit_transform(nX_train)\n",
        "lr_cgrm = Ridge(alpha=90).fit(X_train_cgrm, y_train)\n",
        "X_test_cgrm = cgram.transform(nX_test)\n",
        "print(\"Testing error on character grams with word boundaries: \",lr_cgrm.score(X_test_cgrm, y_test))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing error on character grams with word boundaries:  0.7093071752230775\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVj2KhVCvmWS",
        "colab_type": "text"
      },
      "source": [
        "The character n-grams performed well with 1-3 characters, compared to the other models this was not as good. We also had to do high regularization for this model as there are more no of features and if not regularized properly the model will not generalize well. We again removed the stop words, split the characters by respecting their boundaries. We have taken characters that appeared in more than 3 documents. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAdAMc2JB_ds",
        "colab_type": "code",
        "outputId": "3cd234b3-5a6b-4356-ea3c-b1daba2502e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#tf-idf rescaling\n",
        "\n",
        "tiv = TfidfVectorizer(stop_words=\"english\", tokenizer=LemmaTokenizer())\n",
        "tfidf_X = tiv.fit_transform(nX_train)\n",
        "lr_tfidf = Ridge(alpha=0.5).fit(tfidf_X, y_train)\n",
        "tfidf_X_test = tiv.transform(nX_test)\n",
        "print(\"Testing error on data after tf-idf rescaling: \",lr_tfidf.score(tfidf_X_test, y_test))\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing error on data after tf-idf rescaling:  0.725454744901997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52Wi7mXSwyNB",
        "colab_type": "text"
      },
      "source": [
        "The tf-idf rescaled model performed better with lemmatization. We referred to the following page for adding lemmatization  https://scikit-learn.org/stable/modules/feature_extraction.html .\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfmVb2-iB7my",
        "colab_type": "code",
        "outputId": "151fb94f-45cd-4970-d989-17a9ef877182",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#tuned bag of words model with stop words, different tokenization, removed non-frequent words, stemming\n",
        "\n",
        "vect = CountVectorizer(min_df=3, stop_words=\"english\",lowercase=True, analyzer=stemmed_words, token_pattern=r\"\\b\\w[\\wâ€™]+\\b\")\n",
        "teXt_train = vect.fit_transform(nX_train).todense()\n",
        "teXt_test = vect.transform(nX_test).todense()\n",
        "\n",
        "clf = Ridge(alpha=11).fit(teXt_train, y_train)\n",
        "print(\"testing score for tuned bag of words model: \",clf.score(teXt_test,y_test))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing score for tuned bag of words model:  0.732732383755386\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qllOZLC-xSfz",
        "colab_type": "text"
      },
      "source": [
        "The bag of words model performed better at an alpha of 11, and with word stemming. We also removed the stop words and only included the words that appeared in more than 3 documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xemKz_a1XXoq",
        "colab_type": "text"
      },
      "source": [
        "## **Task 1.4**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpix6w2aXjzg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "38f7bcd4-90d1-4725-838a-03614159cbfc"
      },
      "source": [
        "#adding the tuned bag of words to the other data from task 1.1\n",
        "\n",
        "#making column names for the bag of words data\n",
        "c=[]\n",
        "for i in range(0,teXt_train.shape[1]):\n",
        "  c.append('p'+str(i))\n",
        "\n",
        "X_test1.reset_index(drop=True,inplace=True)\n",
        "\n",
        "Xtr=pd.DataFrame(teXt_train, columns = c)\n",
        "Xte=pd.DataFrame(teXt_test, columns = c)\n",
        "\n",
        "Xtr.reset_index(drop=True,inplace=True)\n",
        "Xte.reset_index(drop=True,inplace=True)\n",
        "\n",
        "#Merging the bag of words data and the rest of the data\n",
        "X_train2 = pd.concat([Xtr, X_train1], axis=1)\n",
        "X_test2 = pd.concat([Xte, X_test1], axis=1)\n",
        "\n",
        "X_train2.reset_index(drop=True,inplace=True)\n",
        "X_test2.reset_index(drop=True,inplace=True)\n",
        "\n",
        "preprocess = make_column_transformer((cat_preprocessing, cat_cols), (cont_preprocessing,cont_cols), ( 'passthrough',c) , (cat_preprocessing_on,cat_cols_on))\n",
        "\n",
        "pipe = Pipeline([(\"pre\",preprocess),(\"regressor\",Ridge())])\n",
        "\n",
        "pipe_grid={\"regressor__alpha\": [15, 20]}\n",
        "grid = GridSearchCV(pipe,param_grid=pipe_grid,cv=2,scoring=\"r2\")\n",
        "grid.fit(X_train2,y_train)\n",
        "\n",
        "print(\"Best alpha value is:\", grid.best_params_)\n",
        "print(\"model with bag of words and rest of the data's score on the test set\", grid.best_estimator_.score(X_test2, y_test))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best alpha value is: {'regressor__alpha': 20}\n",
            "model with bag of words and rest of the data's score on the test set 0.7655129909622163\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxeK2ON6xvqc",
        "colab_type": "text"
      },
      "source": [
        "The n-gram model performed the best of all our model in the task 1.3. The encodings from the bag of words model with the rest of the data performed better than the n-grams model. Adding the rest of the data to the bag of words model actually helped to improve the performance. "
      ]
    }
  ]
}